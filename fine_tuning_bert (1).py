# -*- coding: utf-8 -*-
"""Fine_Tuning_Bert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pUMUKb9m1LkLxnbAElFfzjDmTZ6Goha5
"""

# Install necessary packages
!pip install -q datasets scikit-learn evaluate

!pip install -U transformers

# Import required libraries
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import numpy as np
import evaluate

# 1Ô∏è‚É£ Load Dataset
dataset_name = "ag_news"
dataset = load_dataset(dataset_name)
print(dataset)

train_dataset = dataset["train"]
eval_dataset = dataset["test"]

# 2Ô∏è‚É£ Load tokenizer
model_checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

# Example input string
text = "The stock market saw record gains today."

# Tokenize (convert to input IDs and attention mask)
inputs = tokenizer(text, return_tensors="pt")

# Show tokenized input
print("üì• Tokenized input:")
for k, v in inputs.items():
    print(f"{k}: {v.shape} -> {v}")

# 3Ô∏è‚É£ Tokenize data
def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length")

train_dataset = train_dataset.map(preprocess_function, batched=True)
eval_dataset = eval_dataset.map(preprocess_function, batched=True)

# Load model
num_labels = dataset["train"].features["label"].num_classes
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)


for param in model.base_model.parameters():
    param.requires_grad = False #768*4 params train

# Unfreeze only the pooler layer
for param in model.bert.pooler.parameters():
    param.requires_grad = True

def count_parameters(model):
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return {
        "Total": total_params,
        "Trainable": trainable_params,
        "Frozen": total_params - trainable_params
    }

# Example:
param_counts = count_parameters(model)
print("Model Parameter Counts:")
for k, v in param_counts.items():
    print(f"{k}: {v:,}")

outputs = model(**inputs)
print(outputs)

import evaluate
import numpy as np

# Load the evaluation metric
accuracy_metric = evaluate.load("accuracy")

# Define compute_metrics function
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=1)
    return accuracy_metric.compute(predictions=predictions, references=labels)

# 6Ô∏è‚É£ Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="steps",           # üîÅ Log training loss every N steps
    logging_steps=10,                   # ‚è± Logs every 10 steps
    learning_rate=5e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    report_to="none"
)

# 7Ô∏è‚É£ Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# 8Ô∏è‚É£ Train model
trainer.train()

label_map = dataset["train"].features["label"].int2str  # maps 0 ‚Üí 'World', etc.

import torch

def classify(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=256)
    inputs={k:v.to(model.device) for k,v in inputs.items()}
    with torch.no_grad():
        outputs = model(**inputs)
    predicted_class_id = outputs.logits.argmax(dim=-1).item()
    return label_map(predicted_class_id)

print(classify("The stock market saw record gains today."))       # ‚Üí 'Business'

